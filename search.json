[
  {
    "objectID": "sections/working-with-data.html",
    "href": "sections/working-with-data.html",
    "title": "Working with Data",
    "section": "",
    "text": "Above all, it is crucial to observe that the term “data” serves a different rhetorical function than do sister terms such as “facts” and “evidence.” To put it more precisely, in contrast to these other terms, the semantic function of data is specifically rhetorical.\n\n\nDaniel Rosenberg, “Data Before the Fact” in Raw Data is an Oxymoron\n\n\nLike families, tidy datasets are all alike but every messy dataset is messy in its own way.\n\n\nHadley Wickham, “Tidy Data,” Journal of Statistical Software\n\n\n\nIt would take a long long long time to unpack such a question, so let’s skip to a more salient pair of concerns: How to think of our own research in terms of structured data? How do we go about nudging our objects of study (archives/notes/digital images/PDFs) into forms that allow for computer analysis or visualization? Whether we want to use machine learning to predict patterns or simply want to share maps and digital exhibits online, we need to have some working understanding of our research as data.\nData always comprises a slippery set of concepts, processes, technologies, and products. In light of Daniel Rosenberg’s comment above on the etymology of “data”, consider how even “mechanical” processes like creating and cleaning data are rhetorical, are already engaging in argument – before we even get to visualization or interpretation we are already engaged in issues of representation and modeling the world, opening up possibilities while precluding others.\nBy and large, the data (or rather, datasets) we create or find in the wild are either tabular (think spreadsheet) or graph data (think tree hierarchy or network). However complicated or simple, datasets consist of values that belong to predetermined variables. These variables, in turn, can describe either categorical, continuous, or discontinuous data. We include temporal and geographic data though these data types include special considerations in terms of comparison and mapping.\nData and datasets, in this context, are not only for quantitative research or even for research with highly structured output. Citations and library catalogs, archival finding aids and digital collections of any media, from video to journal articles – these are all examples of data created and/or reused for research across disciplines.\nWhat in your own research relies on grouping objects of study into meaningful categories? What might include quantitative data, whether discontinuous or continuous?\n\n\n\nThe annoying retort would be what is not a data file? If we are not worried about being terribly precise, we can talk usefully about a taxonomy of common formats for sharing datasets. Note that we are not talking about databases here; rather we are concerned with data encoded in text characters and written to a file. As such, these files are subject to the same issues mentioned earlier with regard to other plain text files, particularly invisible characters like carriage returns, spaces, tabs, or other special characters added by software inadvertently.\nTabular data can be represented in terms of columns, rows, and cells. Rows represent observations or instances, which have values corresponding to column variables. Most commonly seen in proprietary formats (Excel or GoogleSheets files) or character delimited text files like CSV (comma separated values) or TSV (tab separated values).\nGraph data is data that can be represented by nodes and edges, roughly speaking. Think popularized images of networks spidering out. A directed graph is data that can be represented by a tree or a hierarchy. Most commonly seen in XML or JSON formats (though these formats can also represent tabular data).\nWhile some of us may only ever open and read spreadsheets, we interact with graph data constantly, whether using the library catalog or searching Facebook or using web-based maps. Websites themselves are directed graph data represented by the browse.\nWhy pay attention to data files? Clearly, we can save ourselves and others headache and heartbreak by being mindful with data files, by keeping folders organized, naming files sanely, having consistent, logical column headers. Beyond that, granting organizations and scholarly journals across disciplines are increasingly interested in underlying datasets. Plus you might want to visit the European Spreadsheet Risks Interest Group (yes, that’s a thing) for a curious, sometimes comic collection of messy data horror stories.",
    "crumbs": [
      "Home",
      "Working with Data"
    ]
  },
  {
    "objectID": "sections/working-with-data.html#what-is-data",
    "href": "sections/working-with-data.html#what-is-data",
    "title": "Working with Data",
    "section": "",
    "text": "It would take a long long long time to unpack such a question, so let’s skip to a more salient pair of concerns: How to think of our own research in terms of structured data? How do we go about nudging our objects of study (archives/notes/digital images/PDFs) into forms that allow for computer analysis or visualization? Whether we want to use machine learning to predict patterns or simply want to share maps and digital exhibits online, we need to have some working understanding of our research as data.\nData always comprises a slippery set of concepts, processes, technologies, and products. In light of Daniel Rosenberg’s comment above on the etymology of “data”, consider how even “mechanical” processes like creating and cleaning data are rhetorical, are already engaging in argument – before we even get to visualization or interpretation we are already engaged in issues of representation and modeling the world, opening up possibilities while precluding others.\nBy and large, the data (or rather, datasets) we create or find in the wild are either tabular (think spreadsheet) or graph data (think tree hierarchy or network). However complicated or simple, datasets consist of values that belong to predetermined variables. These variables, in turn, can describe either categorical, continuous, or discontinuous data. We include temporal and geographic data though these data types include special considerations in terms of comparison and mapping.\nData and datasets, in this context, are not only for quantitative research or even for research with highly structured output. Citations and library catalogs, archival finding aids and digital collections of any media, from video to journal articles – these are all examples of data created and/or reused for research across disciplines.\nWhat in your own research relies on grouping objects of study into meaningful categories? What might include quantitative data, whether discontinuous or continuous?",
    "crumbs": [
      "Home",
      "Working with Data"
    ]
  },
  {
    "objectID": "sections/working-with-data.html#whats-in-a-data-file",
    "href": "sections/working-with-data.html#whats-in-a-data-file",
    "title": "Working with Data",
    "section": "",
    "text": "The annoying retort would be what is not a data file? If we are not worried about being terribly precise, we can talk usefully about a taxonomy of common formats for sharing datasets. Note that we are not talking about databases here; rather we are concerned with data encoded in text characters and written to a file. As such, these files are subject to the same issues mentioned earlier with regard to other plain text files, particularly invisible characters like carriage returns, spaces, tabs, or other special characters added by software inadvertently.\nTabular data can be represented in terms of columns, rows, and cells. Rows represent observations or instances, which have values corresponding to column variables. Most commonly seen in proprietary formats (Excel or GoogleSheets files) or character delimited text files like CSV (comma separated values) or TSV (tab separated values).\nGraph data is data that can be represented by nodes and edges, roughly speaking. Think popularized images of networks spidering out. A directed graph is data that can be represented by a tree or a hierarchy. Most commonly seen in XML or JSON formats (though these formats can also represent tabular data).\nWhile some of us may only ever open and read spreadsheets, we interact with graph data constantly, whether using the library catalog or searching Facebook or using web-based maps. Websites themselves are directed graph data represented by the browse.\nWhy pay attention to data files? Clearly, we can save ourselves and others headache and heartbreak by being mindful with data files, by keeping folders organized, naming files sanely, having consistent, logical column headers. Beyond that, granting organizations and scholarly journals across disciplines are increasingly interested in underlying datasets. Plus you might want to visit the European Spreadsheet Risks Interest Group (yes, that’s a thing) for a curious, sometimes comic collection of messy data horror stories.",
    "crumbs": [
      "Home",
      "Working with Data"
    ]
  },
  {
    "objectID": "sections/tidy-vs-messy.html",
    "href": "sections/tidy-vs-messy.html",
    "title": "Tidy vs. Messy Part I",
    "section": "",
    "text": "Tidy vs. Messy Part I\nWhat is tidy data? To be clear we are using “tidy data” in a capacious sense. Outlined in 2014 by Hadley Wickham, a programmer and statistician, tidy data refers to applying standard ways of optimally formatting data for use with the programming language R. With the recent tidal wave of students, journalists, humanists, artists, and/or scientists interested in engaging data in their research, “tidy data” has taken on a life of its own. Across disciplines and software and file formats awash with new sources of quantitative and qualitative data, who wouldn’t want a Marie Kondo to guide us through?\nThinking tidily is helpful for anybody working with data, but particularly so for those in some humanities and social sciences who may not have had prior need to think in structured ways about their sources. With little practice, these researchers are often thrown into some of the messiest, most inconsistent, and otherwise challenging data to work with. Having a few rules of thumb can be invaluable.\nFor our purposes, tidying up means thinking about data in terms of reusable, machine-readable files. Even better: If the datasets are transparent and able to be audited.\nConcretely, what does this mean? Data entered by hand is going to be inconsistent. Depending on our use case, that may be fine. If we intend to perform computational analysis or create visualizations; however, we will need to address some of that inconsistency because, well, computers are dumb. There is no ambiguity if an extra space appears in the middle of two words, for example, N.  Lombard St. To a machine, that value is utterly different from any other cell in which those same two words might appear, say, N Lombard Street or North Lombard. But as this example might suggest, how can we generalize when “each dataset is messy in its own way”, often requiring context and data specific remediation?\nFor our workshop we are going to be using historical data derived from an 1847 Quaker census of African Americans living the Philadelphia area, found in Friends Historical Library. More context about the dataset is available in the GitHub repository as well as in the companion online exhibit.\nWhat does the file consist of? A strategy that is often of use is to explore a dataset before getting bogged down by line level details. We might use a tool like the web application WTFcsv, part of databasic.io to get started.",
    "crumbs": [
      "Home",
      "Tidy Data vs. Messy Data"
    ]
  },
  {
    "objectID": "sections/summary.html",
    "href": "sections/summary.html",
    "title": "What we’ve learned",
    "section": "",
    "text": "What we’ve learned\nYou are now familiar with how to navigate the program OpenRefine but also some basic, high-level concepts about data in general. Regardless of whether you’re getting data together to do sophisticated analysis, to put batches of digitized of documents on the web, or to place pins on a map.\nIn this session, we learned how:\n\nhow to filter, facet, and sort in OpenRefine\nhow to import and export datasets\nhow to trim whitespace and cluster values for consistency\nhow to transform columns\nhow to think about structured data generally using tidy data inspired concepts"
  },
  {
    "objectID": "sections/introducing-openrefine.html",
    "href": "sections/introducing-openrefine.html",
    "title": "What is OpenRefine?",
    "section": "",
    "text": "OpenRefine is cross-platform, open-source software that allows users to clean and transform messy data. Originally supported by Google, it is now maintained by a large user community. OpenRefine is by no means the only or best way to work with data; however, it strikes an unusual balance between working in proprietary tools for a broad audience (like Microsoft Excel or GoogleSheets) on the one hand, and on the other, straight programming for data science (R Studio, Python Notebooks).\nIt allows users to bring in messy data easily without requiring preliminary transformation. It allows for granular auditing of any changes. It combines (relatively) easy to use suite of features with the ability to write custom scripts when needed. It also allows users to parse with lots and lots of data. Microsoft Excel, for example, has a maximum of 1,048,576 rows by 16,384 columns.\nA couple things to note about OpenRefine: The software was originally written for the web so it runs in the browser rather than its own window. Rather than opening and saving files, it imports data into a “project” from which we can export versions of data suited to our needs.\nIf you haven’t yet, be sure to download and install OpenRefine according to the instructions on the installations page.\n\n\nLike git, OpenRefine is not structured around discrete files that you open and save. Data is imported into a project without affecting the original file. Changes made to that data are bundled together into another file. Rather than saving over or saving multiple copies of data (i.e., datafile_final-reallyfinal-FINAL.xls) projects allow users to export derivative files with fine-grained control over how those derivatives should be formed.\n\nFrom the Create Project option, select Choose Files\n\n\n\n\nopenrefine: create a project screenshot\n\n\n\nHaving selected the data file, create a project name in the top bar then select Create Project. You may also choose from an array of options regarding how to parse that data. Note that you can import both tabular data like CSV’s or Excel files as well as nested formats, like JSON or XML.\n\n\n\n\nopenrefine: create a project options screenshot\n\n\n\n\n\nFor our workshop we are going to be using historical data derived from an 1847 Quaker census of African Americans living the Philadelphia area, found in Friends Historical Library. More context about the dataset is available in the GitHub repository as well as in the companion online exhibit.\nNow create an OpenRefine project by selecting Create Project after which you have two options:\n\nSelect This Computer, browse, and navigate to wherever you’ve saved the file sofaac-raw-export.csv\nOr select Web Addresses (URLs) and enter the URL for the dataset (https://raw.githubusercontent.com/swat-ds/datasets/main/1847census/sofaac-raw-export.csv)\n\nIn the next screen go ahead and rename the project in the top bar. For now, let’s stick with the default settings and voilà, our first project.",
    "crumbs": [
      "Home",
      "Introducing OpenRefine"
    ]
  },
  {
    "objectID": "sections/introducing-openrefine.html#importing-data",
    "href": "sections/introducing-openrefine.html#importing-data",
    "title": "What is OpenRefine?",
    "section": "",
    "text": "Like git, OpenRefine is not structured around discrete files that you open and save. Data is imported into a project without affecting the original file. Changes made to that data are bundled together into another file. Rather than saving over or saving multiple copies of data (i.e., datafile_final-reallyfinal-FINAL.xls) projects allow users to export derivative files with fine-grained control over how those derivatives should be formed.\n\nFrom the Create Project option, select Choose Files\n\n\n\n\nopenrefine: create a project screenshot\n\n\n\nHaving selected the data file, create a project name in the top bar then select Create Project. You may also choose from an array of options regarding how to parse that data. Note that you can import both tabular data like CSV’s or Excel files as well as nested formats, like JSON or XML.\n\n\n\n\nopenrefine: create a project options screenshot",
    "crumbs": [
      "Home",
      "Introducing OpenRefine"
    ]
  },
  {
    "objectID": "sections/introducing-openrefine.html#importing-sample-data",
    "href": "sections/introducing-openrefine.html#importing-sample-data",
    "title": "What is OpenRefine?",
    "section": "",
    "text": "For our workshop we are going to be using historical data derived from an 1847 Quaker census of African Americans living the Philadelphia area, found in Friends Historical Library. More context about the dataset is available in the GitHub repository as well as in the companion online exhibit.\nNow create an OpenRefine project by selecting Create Project after which you have two options:\n\nSelect This Computer, browse, and navigate to wherever you’ve saved the file sofaac-raw-export.csv\nOr select Web Addresses (URLs) and enter the URL for the dataset (https://raw.githubusercontent.com/swat-ds/datasets/main/1847census/sofaac-raw-export.csv)\n\nIn the next screen go ahead and rename the project in the top bar. For now, let’s stick with the default settings and voilà, our first project.",
    "crumbs": [
      "Home",
      "Introducing OpenRefine"
    ]
  },
  {
    "objectID": "sections/extra-credit.html",
    "href": "sections/extra-credit.html",
    "title": "Extra credit",
    "section": "",
    "text": "Now that the data is a little tidier, what else can we do to it? One common transformation is to augment the data or bring in other data to make it more useful. One way of augmenting data is to geocode or return the latitudinal and longitudinal coordinates for mapping purposes. How would we go about turning our street addresses to contemporary coordinates?\n\nCluster and edit to normalize our addresses the best we can.\nReconcile historical addresses.\nGeocode addresses.\nClean up coordinates.\n\n\n\nSee exploring OpenRefine to revisit how to cluster and reconcile similar values.\n\n\n\nWhile there are more sophisticated methods and more elaborate historical gazetteers, we could informally begin with historical alleys and 18th century street-name changes from philahistory.net.\n\n\n\nGeocoding is a common enough use case that there are many web platforms that offer the service. Edit Column -&gt; Add column by fetching URLs… using a geocoding service.\nWe’ll use Mapquest’s service in this case. The URL for this service is http://www.mapquestapi.com/geocoding/v1/address?key=[API KEY]&location=ADDRESS+philadelphia+pa.\nNote that you will need to register for your own key in order use their Mapquest’s servers.\nThe OpenRefine command we can use to return the URL follows the pattern \"[base URL]\".replace(ADDRESS, value) – in other words take the base URL and replace some part of it with the current address.\nInstead of simply replacing the unedited current address value, we’ll get better results if we replace the spaces with plus signs – value.replace(' ','+').\nWe can add a couple more parameters to improve accuracy like a bounding box for the Philadelphia area, we end up with following (awful, unwieldy) string.\n\"http://www.mapquestapi.com/geocoding/v1/address?key=[API KEY]&boundingBox=39.83,-75.32,40.12,-74.98&thumbMaps=false&location=ADDRESS+philadelphia+pa\".replace('ADDRESS',value.replace(' ','+'))\nNote that you will need to replace the [API KEY] with your API key.\n\n\n\nFinally, you’ll have noticed that the service returns much more than simply the coordinates. How to unpack this?\nThe result is in JSON, which mean we can parse this as a data object, navigating down the result like a tree. To select the first result, for example, we’d select parseJson(value).results. To select the first result parseJson(value).results[0]. And finally to select the first location within the first result: parseJson(value).results[0].locations[0]. Square bracket notation indicates the index or position within a list. The first position is commonly indicated by a 0 in most programming languages.\nPutting it all together – to return the longitudinal and the latitudinal coordinates separated by a comma, we’ll can use the command:\nparseJson(value).results[0].locations[0].latLng.lat + ',' + parseJson(value).results[0].locations[0].latLng.lng\n\n\n\nFinally, how would we get this out of OpenRefine?\nSelect Export -&gt; Templating … and you’ll be presented with quite a lot of control of how you might export the data as JSON.\n\n\n\nOpenRefine Templating Export menu\n\n\nGeojson is a strict format so you will need to include the following Prefix.\n{\n  \"type\": \"FeatureCollection\",\n  \"features\" : [\nAnd we’ll want to replace the Row Template with the following. Note how the preview in the right column updates. What do you think the code {jsonize(cells[\"COLUMN NAME\"].value)} does?\n   {\n\n      \"type\": \"Feature\",\n      \"geometry\": {\n          \"type\": \"Point\",\n          \"coordinates\": [{{ cells[\"Coordinates\"].value }}]\n      },\n\n      \"properties\": {\n      \"Residence\" : {{jsonize(cells[\"Residence\"].value)}},\n      \"Name\" : {{jsonize(cells[\"Name\"].value)}}\n      }\n    }\n\n\n\nTo test out our data, we can open the file in a geojson web application like geojson.io. The open file option is in the top left corner. What should the output look like in the end? Here is an example of the output.\nWhat do you notice? What happens if you edit the data that appears in the right sidebar? Are there any outliers? With a properly formatted Geojson dataset, we can now take advantage of any number of mapping platforms."
  },
  {
    "objectID": "sections/extra-credit.html#cluster-and-edit",
    "href": "sections/extra-credit.html#cluster-and-edit",
    "title": "Extra credit",
    "section": "",
    "text": "See exploring OpenRefine to revisit how to cluster and reconcile similar values."
  },
  {
    "objectID": "sections/extra-credit.html#reconcile-historical-addresses",
    "href": "sections/extra-credit.html#reconcile-historical-addresses",
    "title": "Extra credit",
    "section": "",
    "text": "While there are more sophisticated methods and more elaborate historical gazetteers, we could informally begin with historical alleys and 18th century street-name changes from philahistory.net."
  },
  {
    "objectID": "sections/extra-credit.html#geocode-addresses",
    "href": "sections/extra-credit.html#geocode-addresses",
    "title": "Extra credit",
    "section": "",
    "text": "Geocoding is a common enough use case that there are many web platforms that offer the service. Edit Column -&gt; Add column by fetching URLs… using a geocoding service.\nWe’ll use Mapquest’s service in this case. The URL for this service is http://www.mapquestapi.com/geocoding/v1/address?key=[API KEY]&location=ADDRESS+philadelphia+pa.\nNote that you will need to register for your own key in order use their Mapquest’s servers.\nThe OpenRefine command we can use to return the URL follows the pattern \"[base URL]\".replace(ADDRESS, value) – in other words take the base URL and replace some part of it with the current address.\nInstead of simply replacing the unedited current address value, we’ll get better results if we replace the spaces with plus signs – value.replace(' ','+').\nWe can add a couple more parameters to improve accuracy like a bounding box for the Philadelphia area, we end up with following (awful, unwieldy) string.\n\"http://www.mapquestapi.com/geocoding/v1/address?key=[API KEY]&boundingBox=39.83,-75.32,40.12,-74.98&thumbMaps=false&location=ADDRESS+philadelphia+pa\".replace('ADDRESS',value.replace(' ','+'))\nNote that you will need to replace the [API KEY] with your API key."
  },
  {
    "objectID": "sections/extra-credit.html#clean-up-coordinates",
    "href": "sections/extra-credit.html#clean-up-coordinates",
    "title": "Extra credit",
    "section": "",
    "text": "Finally, you’ll have noticed that the service returns much more than simply the coordinates. How to unpack this?\nThe result is in JSON, which mean we can parse this as a data object, navigating down the result like a tree. To select the first result, for example, we’d select parseJson(value).results. To select the first result parseJson(value).results[0]. And finally to select the first location within the first result: parseJson(value).results[0].locations[0]. Square bracket notation indicates the index or position within a list. The first position is commonly indicated by a 0 in most programming languages.\nPutting it all together – to return the longitudinal and the latitudinal coordinates separated by a comma, we’ll can use the command:\nparseJson(value).results[0].locations[0].latLng.lat + ',' + parseJson(value).results[0].locations[0].latLng.lng"
  },
  {
    "objectID": "sections/extra-credit.html#exporting-geojson",
    "href": "sections/extra-credit.html#exporting-geojson",
    "title": "Extra credit",
    "section": "",
    "text": "Finally, how would we get this out of OpenRefine?\nSelect Export -&gt; Templating … and you’ll be presented with quite a lot of control of how you might export the data as JSON.\n\n\n\nOpenRefine Templating Export menu\n\n\nGeojson is a strict format so you will need to include the following Prefix.\n{\n  \"type\": \"FeatureCollection\",\n  \"features\" : [\nAnd we’ll want to replace the Row Template with the following. Note how the preview in the right column updates. What do you think the code {jsonize(cells[\"COLUMN NAME\"].value)} does?\n   {\n\n      \"type\": \"Feature\",\n      \"geometry\": {\n          \"type\": \"Point\",\n          \"coordinates\": [{{ cells[\"Coordinates\"].value }}]\n      },\n\n      \"properties\": {\n      \"Residence\" : {{jsonize(cells[\"Residence\"].value)}},\n      \"Name\" : {{jsonize(cells[\"Name\"].value)}}\n      }\n    }"
  },
  {
    "objectID": "sections/extra-credit.html#check-your-work",
    "href": "sections/extra-credit.html#check-your-work",
    "title": "Extra credit",
    "section": "",
    "text": "To test out our data, we can open the file in a geojson web application like geojson.io. The open file option is in the top left corner. What should the output look like in the end? Here is an example of the output.\nWhat do you notice? What happens if you edit the data that appears in the right sidebar? Are there any outliers? With a properly formatted Geojson dataset, we can now take advantage of any number of mapping platforms."
  },
  {
    "objectID": "sections/exploring-openrefine.html",
    "href": "sections/exploring-openrefine.html",
    "title": "Exploring an OpenRefine project",
    "section": "",
    "text": "Take a moment to explore our new project. What do you notice? There are many many features, but let’s start with three: sorting, filtering, and faceting.\n\n\nSelect the caret to the left of the Name column and select sort. This is a useful way of exploring data but does not permanently rearrange the data.\nIn order to reorder the data permanently, notice the new Sort option that appears in the top bar. Selecting the caret beside the new Sort option gives you the option to permanently apply a new order to the underlying data.\n\n\n\nFor the column labeled Occupation of females compensation, select Text filter. In the menu in the sidebar, enter an occupation.\nIn how many residences are there recorded female washers? How many males are employed as washers?\n\n\n\nFor the column labeled Rent of house room, let’s filter the census data based on how much rent tenants were paying.\nFirst, we’ll want to convert the column to numeric data by selecting Edit Cells -&gt; Common Transforms -&gt; To number.\nNote that not all rows were converted. Why not?\nFrom the same column, select Facet -&gt; Numeric facet.\nWhat happens when you slide the minimum and maximum selectors that appear in the sidebar?\n\n\n\n\n\nOne common issue that can cause issues further on down the road are invisible characters – like spaces, tabs, and carriage returns. Let’s trim these annoying invisible characters.\nFor Residence, select Edit Cells -&gt; Common Transforms -&gt; Trim leading and trailing whitespace.\nNext select Edit Cells -&gt; Common Transforms -&gt; Collapse consecutive whitespace. What do you imagine that step does?\n\n\n\ncolumn edit cells common transforms submenu in openrefine\n\n\n\n\n\nWhether in the 1847 handwritten entries themselves or in the process of transcribing them to a digital file, the census data we have is far from consistent. What would be great is the ability to make sure every street name and address descriptor was consistent–across all 4,308 rows.\nHere we are going to explore one of several ways we can use OpenRefine to guess what values are most like each other in order to make values more consistent.\nFor Residence, select Edit Cells -&gt; Cluster and edit….\nOpenRefine suggests changes and by checking the checkbox beside those suggestions, we can replace all the resulting cells with what is in the textbox to the right.\nNotice that you can Select All or Unselect All from the bottom of the menu.\nSelect Merge selected & Re-Cluster. We can repeat the process until as many times as we want. Note there are other algorithms that will allow us to get slightly different results and that each algorithm has a suite of options to tweak. Here’s a more in-depth treatment of those options.",
    "crumbs": [
      "Home",
      "Exploring OpenRefine"
    ]
  },
  {
    "objectID": "sections/exploring-openrefine.html#transforming-cells",
    "href": "sections/exploring-openrefine.html#transforming-cells",
    "title": "Exploring an OpenRefine project",
    "section": "",
    "text": "One common issue that can cause issues further on down the road are invisible characters – like spaces, tabs, and carriage returns. Let’s trim these annoying invisible characters.\nFor Residence, select Edit Cells -&gt; Common Transforms -&gt; Trim leading and trailing whitespace.\nNext select Edit Cells -&gt; Common Transforms -&gt; Collapse consecutive whitespace. What do you imagine that step does?\n\n\n\ncolumn edit cells common transforms submenu in openrefine\n\n\n\n\n\nWhether in the 1847 handwritten entries themselves or in the process of transcribing them to a digital file, the census data we have is far from consistent. What would be great is the ability to make sure every street name and address descriptor was consistent–across all 4,308 rows.\nHere we are going to explore one of several ways we can use OpenRefine to guess what values are most like each other in order to make values more consistent.\nFor Residence, select Edit Cells -&gt; Cluster and edit….\nOpenRefine suggests changes and by checking the checkbox beside those suggestions, we can replace all the resulting cells with what is in the textbox to the right.\nNotice that you can Select All or Unselect All from the bottom of the menu.\nSelect Merge selected & Re-Cluster. We can repeat the process until as many times as we want. Note there are other algorithms that will allow us to get slightly different results and that each algorithm has a suite of options to tweak. Here’s a more in-depth treatment of those options.",
    "crumbs": [
      "Home",
      "Exploring OpenRefine"
    ]
  },
  {
    "objectID": "sections/contributing.html",
    "href": "sections/contributing.html",
    "title": "Contributing to the curriculum",
    "section": "",
    "text": "There are many ways to contribute back to the curriculum, from creating issues that suggest improvements to specific modules to creating entirely new sessions. This section describes various modes of interaction with the DHRI curriculum materials, including forking repositories to your account and creating issues and pull requests.\n\n\nA critical and relatively accessible way to contribute back to the DHRI curriculum is to create an issue on GitHub. While reading DHRI curriculum on GitHub, you may encounter inconsistencies, typos, sections that need clarification, or typographical errors. When this happens, you can open an issue to let us know about the problem or potential improvement.\nTo open an issue, you must be in a GitHub repository. In the menu bar at the top center of the page, click the Issues tab.\n.\nAfter clicking the issues tab, you will see a screen with the currently open issues, if any. To create a new issue, click the large green button on the right of the page.\n\n\n\nImage showing where the Create New Issue button is in the Issues tab on GitHub\n\n\nOn the New Issue page, there is only one required field—the issue title. However, consider leaving a detailed explanation of the issue, including which file you encountered it, if relevant. You can use markdown formatting in the body of your issue.\nTo complete your issue, click the green button labelled Submit New Issue.\n.\n\n\n\nBy deffault, you will not have access to directly modify repositories in the DHRI curriculum. In the world of free and open source software, this is common—usually, only the creator of a repository and certain core contributors can add and modify files in a repository. However, this doesn’t mean that you cannot modify the curriculum and make it your own.\nIf you wish to create your own version of the curriculum for your own workshop or Institute, the first step is to fork the relevant repository. That means to make a copy of it under your own account on GitHub, one in which you have access to modify files. Once you have forked a repository, you can create a new lesson based on it. Because the curriculum is under a Creative Commons license, you may use any module as the basis for a new lesson and share freely, as long as you provide attribution to Graduate Center Digital Initiatives and share under a the same license.\nTo fork a repository to your account, click the Fork button at the top right of any repository on GitHub.\n\n\n\nImage showing where fork button is on GitHub\n\n\nAfter clicking the fork button, if you’re a member of an organization on GitHub, you may see a screen asking you to which account you wish to fork the repository. Choose your personal account. If you are not a member of an organization, you will not see this screen.\nOnce you have clicked Fork and (possibly) chosen an account or organization, you will see a screen showing you that the fork is in progress:\n\n\n\nImage showing the fork in progress\n\n\nOnce the fork is complete, you will be directed to a screen showing the repository in your own account.\n\n\n\nImage showing the forked repository in your own account\n\n\nAt this point, you can begin to create your own version, or fork, of the session by cloning this repository to your computer, making changews, and pushing them back up to GitHub.\n\n\n\nA pull request allows you to make a specific suggestion or set of suggestions on a repository that can be accepted by the owner of that repository. Basically, you are creating a new version of the target repository, pointing to it, and saying, “This is what I think your repository should look like.” If the owner of the target repository agrees with you, they can click a button on GitHub that allows them to incorporate the changes.\nTo do a pull request takes a few steps:\n\nFOrk the repository as above.\nClone the forked repository.\nMake the changes you intend to propose locally.\nAdd, commit, and push your changes to the forked repository.;\nUsing the GitHub interface, initiate the pull requestA pull request allows you to make a specific suggestion or set of suggestions on a repository that can be accepted by the owner of that repository. Basically, you are creating a new version of the target repository, pointing to it, and saying, “This is what I think your repository should look like.” If the owner of the target repository agress with you, they can click a button on GitHub that allows them to incorporate the changes.\n\nA pull request allows you to make a specific suggestion or set of suggestions on a repository that can be accepted by the owner of that repository. Basically, you are creating a new version of the target repository, pointing to it, and saying, “This is what I think your repository should look like.” If the owner of the target repository agrees with you, they can click a button on GitHub that allows them to incorporate the changes.\nLet’s step through these steps in more detail.\nFirst, fork the repository to which you wish to submit a pull request. Instructions for forking are available in the previous section on forking.\nNext, clone the forked repository to your computer by clicking the Clone or Download button, copying the link, and using the\ngit clone &lt;copied-link&gt;\ncommand in your terminal.\nOnce you’ve cloned the repository, use a text editor such as VS Code to make the changes you wish to propose to the target repository. You can make any number of changes in any number of files.\nOnce you have finished editing, use the add command to stage your changes, commit them, and then push them back to your forked repository on GitHub.\ngit add -A\ngit commit -m \"Changes proposed for pull request\"\ngit push\nNote that you can add files one by one instead of using the -A flag to add all files, and that you should use a more specific and description commit message than the generic one provided above.\nOnce your changes have been pushed to GitHub, open the web interface to the repository and refresh the page. It’s easy to miss it in the interface, but there will be a notification about the commit you just made. Within that notification, click the Pull Request button.\n\n\n\nSmall button for pull request in the GitHub interface\n\n\nIf you can’t find the notification, or if it disappears, you can also begin the pull request process by clicking the Pull Request tab and clicking the green New Pull Request button.\nOnce the pull request is initiated, you will encounter a screen that compares the changes between your forked repository and the target repository.\n\n\n\nImage showing the screen that compares changes in the GitHub interface\n\n\nAfter reviewing, click the green Create Pull Request button. You will have a chance to add a comment, and the interface is similar to the interface when creating issues.\n\n\n\nImage showing the message box after creating a pull request\n\n\nAfter adding a descriptive message, click the green button to finalize the pull request. You will be taken to a view of the completed pull request.\nNow it’s up to the repository owner if they wish to accept the request, initiate a conversation with you, or—which sometimes happens—ignore or reject your request. In the case of the DHRI curriculum, we may accept some requests right away, and ask for clarification on other occasions."
  },
  {
    "objectID": "sections/contributing.html#creating-issues",
    "href": "sections/contributing.html#creating-issues",
    "title": "Contributing to the curriculum",
    "section": "",
    "text": "A critical and relatively accessible way to contribute back to the DHRI curriculum is to create an issue on GitHub. While reading DHRI curriculum on GitHub, you may encounter inconsistencies, typos, sections that need clarification, or typographical errors. When this happens, you can open an issue to let us know about the problem or potential improvement.\nTo open an issue, you must be in a GitHub repository. In the menu bar at the top center of the page, click the Issues tab.\n.\nAfter clicking the issues tab, you will see a screen with the currently open issues, if any. To create a new issue, click the large green button on the right of the page.\n\n\n\nImage showing where the Create New Issue button is in the Issues tab on GitHub\n\n\nOn the New Issue page, there is only one required field—the issue title. However, consider leaving a detailed explanation of the issue, including which file you encountered it, if relevant. You can use markdown formatting in the body of your issue.\nTo complete your issue, click the green button labelled Submit New Issue.\n."
  },
  {
    "objectID": "sections/contributing.html#forking",
    "href": "sections/contributing.html#forking",
    "title": "Contributing to the curriculum",
    "section": "",
    "text": "By deffault, you will not have access to directly modify repositories in the DHRI curriculum. In the world of free and open source software, this is common—usually, only the creator of a repository and certain core contributors can add and modify files in a repository. However, this doesn’t mean that you cannot modify the curriculum and make it your own.\nIf you wish to create your own version of the curriculum for your own workshop or Institute, the first step is to fork the relevant repository. That means to make a copy of it under your own account on GitHub, one in which you have access to modify files. Once you have forked a repository, you can create a new lesson based on it. Because the curriculum is under a Creative Commons license, you may use any module as the basis for a new lesson and share freely, as long as you provide attribution to Graduate Center Digital Initiatives and share under a the same license.\nTo fork a repository to your account, click the Fork button at the top right of any repository on GitHub.\n\n\n\nImage showing where fork button is on GitHub\n\n\nAfter clicking the fork button, if you’re a member of an organization on GitHub, you may see a screen asking you to which account you wish to fork the repository. Choose your personal account. If you are not a member of an organization, you will not see this screen.\nOnce you have clicked Fork and (possibly) chosen an account or organization, you will see a screen showing you that the fork is in progress:\n\n\n\nImage showing the fork in progress\n\n\nOnce the fork is complete, you will be directed to a screen showing the repository in your own account.\n\n\n\nImage showing the forked repository in your own account\n\n\nAt this point, you can begin to create your own version, or fork, of the session by cloning this repository to your computer, making changews, and pushing them back up to GitHub."
  },
  {
    "objectID": "sections/contributing.html#creating-a-pull-request",
    "href": "sections/contributing.html#creating-a-pull-request",
    "title": "Contributing to the curriculum",
    "section": "",
    "text": "A pull request allows you to make a specific suggestion or set of suggestions on a repository that can be accepted by the owner of that repository. Basically, you are creating a new version of the target repository, pointing to it, and saying, “This is what I think your repository should look like.” If the owner of the target repository agrees with you, they can click a button on GitHub that allows them to incorporate the changes.\nTo do a pull request takes a few steps:\n\nFOrk the repository as above.\nClone the forked repository.\nMake the changes you intend to propose locally.\nAdd, commit, and push your changes to the forked repository.;\nUsing the GitHub interface, initiate the pull requestA pull request allows you to make a specific suggestion or set of suggestions on a repository that can be accepted by the owner of that repository. Basically, you are creating a new version of the target repository, pointing to it, and saying, “This is what I think your repository should look like.” If the owner of the target repository agress with you, they can click a button on GitHub that allows them to incorporate the changes.\n\nA pull request allows you to make a specific suggestion or set of suggestions on a repository that can be accepted by the owner of that repository. Basically, you are creating a new version of the target repository, pointing to it, and saying, “This is what I think your repository should look like.” If the owner of the target repository agrees with you, they can click a button on GitHub that allows them to incorporate the changes.\nLet’s step through these steps in more detail.\nFirst, fork the repository to which you wish to submit a pull request. Instructions for forking are available in the previous section on forking.\nNext, clone the forked repository to your computer by clicking the Clone or Download button, copying the link, and using the\ngit clone &lt;copied-link&gt;\ncommand in your terminal.\nOnce you’ve cloned the repository, use a text editor such as VS Code to make the changes you wish to propose to the target repository. You can make any number of changes in any number of files.\nOnce you have finished editing, use the add command to stage your changes, commit them, and then push them back to your forked repository on GitHub.\ngit add -A\ngit commit -m \"Changes proposed for pull request\"\ngit push\nNote that you can add files one by one instead of using the -A flag to add all files, and that you should use a more specific and description commit message than the generic one provided above.\nOnce your changes have been pushed to GitHub, open the web interface to the repository and refresh the page. It’s easy to miss it in the interface, but there will be a notification about the commit you just made. Within that notification, click the Pull Request button.\n\n\n\nSmall button for pull request in the GitHub interface\n\n\nIf you can’t find the notification, or if it disappears, you can also begin the pull request process by clicking the Pull Request tab and clicking the green New Pull Request button.\nOnce the pull request is initiated, you will encounter a screen that compares the changes between your forked repository and the target repository.\n\n\n\nImage showing the screen that compares changes in the GitHub interface\n\n\nAfter reviewing, click the green Create Pull Request button. You will have a chance to add a comment, and the interface is similar to the interface when creating issues.\n\n\n\nImage showing the message box after creating a pull request\n\n\nAfter adding a descriptive message, click the green button to finalize the pull request. You will be taken to a view of the completed pull request.\nNow it’s up to the repository owner if they wish to accept the request, initiate a conversation with you, or—which sometimes happens—ignore or reject your request. In the case of the DHRI curriculum, we may accept some requests right away, and ask for clarification on other occasions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Cleaning with OpenRefine",
    "section": "",
    "text": "In order to begin thinking about digital methods, scholars must first make the conceptual leap toward thinking about their research as data. How do we get at the data in our research and how do we make it useful and usable by machines? What are some of the promises (and perils) of reframing research as data? By the end of the session, we’ll be introduced to strategies and tools for taking very different kinds of information and creating well-formed data, data that can then be used for analysis or visualization.\nBy way of introduction to working with data, we are going to focus on a) conceptually how data is structured using the tidy data framework and b) practically speaking, how to make it useable by other humans and machines with the program OpenRefine.\nIn this session, we will:\nGet Started",
    "crumbs": [
      "Home",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#sections",
    "href": "index.html#sections",
    "title": "Data Cleaning with OpenRefine",
    "section": "Sections",
    "text": "Sections\n\nWorking with Data\nTidy vs Messy Data\nIntroducing OpenRefine\nExploring OpenRefine\nTidy vs Messy Data Part II\nTransforming Columns\nExporting Data\nDeduplicating Rows\nGlossary",
    "crumbs": [
      "Home",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Data Cleaning with OpenRefine",
    "section": "Resources",
    "text": "Resources\nOpenRefine Introductory Video Tutorials\nProgramming Historian’s Cleaning Data with OpenRefine\nTidy Data\nGlossary\n\nCurrent editor: Alice McGrath Past editors: Anna Lacy and James Truitt Original author: Nabil Kashyap\n\n\n\nCreative Commons License\n\n\nDigital Research Institute (DRI) Curriculum by Graduate Center Digital Initiatives is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. Based on a work at https://github.com/DHRI-Curriculum. When sharing this material or derivative works, preserve this paragraph, changing only the title of the derivative work, or provide comparable attribution.",
    "crumbs": [
      "Home",
      "Overview"
    ]
  },
  {
    "objectID": "sections/checklist.html",
    "href": "sections/checklist.html",
    "title": "Module Checklist",
    "section": "",
    "text": "Module Checklist\nEvery module in the curriculum should have these elements:\n\nA dedicated repository on GitHub\nA README.md file.\nFour elements present in the README.md file:\n\na h1 level heading with the name of the tutorial\na short paragraph with an accessible description of the technique\na short paragraph with the goals or outcomes of the workshop\na table of contents with links to the topics in the tutorial\n\nAn introductory page describing the technique or technology in more detail, ideally also explaining the purpose or value of the approach\nA page or pages with installation or setup instructions\nA page or set of pages for each major concept or milestone in the tutorial.\n“&lt;&lt;&lt; Previous” and “Next &gt;&gt;&gt;” buttons on each page that indicate the path a reader will take through the tutorial.\nCode segments should be denoted using the markdown syntax for code and interspersed with explanations. Large blocks of unexplained code should be avoided, but if they’re necessary link to a raw text file or Jupyter notebook within the explanatory text.\nThe tutorial should ideally build incrementally toward a larger goal or project and produce some kind of product, whether a visualization, webpage, repository, analysis, app, or draft writing/proposal. The tutorial should be iterative and purposeful—don’t just cover material to cover material.\nWhen possible, prefer techniques or approaches from other DRI tutorials.\nEverything substantially covered verbally or through slides in a tutorial should appear in the markdown version. Someone not present at the event or who can not hear or see the materials presented should still, in theory, be able to follow based on the materials provided in the markdown tutorial.\nIf many topics or headings are included in the tutorial (more than 6), the markdown files other than the README.md should be placed in a folder within the repository labelled sections or similar and linked from there. (Having too many files in the files section on GitHub looks messy and is visually overwhelming.)\nIf applicable, provide a resources section with links to sources, books, tools, or other tutorials at the end of your markdown tutorial.\nWhile you have the option to create tutorials under your own name on GitHub, when finished or when you reach a major milestone with your tutorial, fork it to the GCDF account on GitHub for reference by future Fellows."
  },
  {
    "objectID": "sections/deduplicating.html",
    "href": "sections/deduplicating.html",
    "title": "Deduplicating rows",
    "section": "",
    "text": "We would normally use Facets-&gt;Customized facets-&gt;Duplicates facet in order to identify duplicate rows; however, no single column in this set is a unique identifier. Some residences have the same name. Some have the same address. How to identify and delete duplicate rows when there is no identifier? Admittedly, this process is convoluted and is decidedly not the slickest feature of OpenRefine.\nOne way to approximate an identifier is to glob the values of a row together. Unique residences may have the same name. They may even have the same street address as another residence. But they would not have the same “Name” + “Residence” + “Number in family” + … + “Remarks”.\n\n\n\nUnder the Name column, select Edit column -&gt; Add column based on this column…. Name this column temp.\nUsing GREL, we can access the value of other cells in the current row by using the cells variable and indicating which column, i.e., cells['Residence']. This object has multiple attributes, so we’ll want to specify that we want the string value, as in cells['Residence'].value.\nTo smoosh together several values, we can literally just “add” them together: cells['Name'].value + cells['Residence'].value + cells['Number in family'].value + cells['Remarks'].value.\nSort on this new column. In order to reorder the underlying data, select Sort -&gt; Reorder rows permanently from the top bar menu.\nNow we blank cells with the same values in our new temp column by selecting Edit cells -&gt; Blank down.\nNow facet on blank values by selecting Facet -&gt; Customized facets -&gt; Facet by blank. Then select the ‘true’ in the sidebar to limit to blank values.\nNow let’s mark all these rows temporarily. OpenRefine let’s us star any row or rows in order to mark them for further action. Select All -&gt; Edit rows -&gt; Star rows. Now go ahead and clear all facets in the left sidebar.\nFinally, to actually remove duplicate rows, select All -&gt; Facet -&gt;Facet by star. Now select All -&gt; Edit rows -&gt; Remove all matching rows.\n\n\n\nOnce we’ve deduplicated, let’s not lose track of our unique rows or “observations.” As we learned in the last section, something fundamental to tidying data is being able to consistently identify observations (i.e., rows) across different tables. As we also learned in the last section, what we thought of as one table, if we are thinking to tidily, should be split into several according the question we’d like to answer.\nUnder the Name column, select Edit column -&gt; Add column based on this column…\nTo access the index of the current row, we can use the row variable and get the index value, i.e., row.index.\nOptionally, if you want to ensure each identifier is a four digit number, we can apply the fancier formula: \"0000\"[0,4-row.index.length()] + row.index\nNow select Edit column and move this new column to the beginning of the table.",
    "crumbs": [
      "Home",
      "Deduplicating rows"
    ]
  },
  {
    "objectID": "sections/deduplicating.html#identifying-duplicate-rows",
    "href": "sections/deduplicating.html#identifying-duplicate-rows",
    "title": "Deduplicating rows",
    "section": "",
    "text": "We would normally use Facets-&gt;Customized facets-&gt;Duplicates facet in order to identify duplicate rows; however, no single column in this set is a unique identifier. Some residences have the same name. Some have the same address. How to identify and delete duplicate rows when there is no identifier? Admittedly, this process is convoluted and is decidedly not the slickest feature of OpenRefine.\nOne way to approximate an identifier is to glob the values of a row together. Unique residences may have the same name. They may even have the same street address as another residence. But they would not have the same “Name” + “Residence” + “Number in family” + … + “Remarks”.",
    "crumbs": [
      "Home",
      "Deduplicating rows"
    ]
  },
  {
    "objectID": "sections/deduplicating.html#create-a-row-based-on-other-columns",
    "href": "sections/deduplicating.html#create-a-row-based-on-other-columns",
    "title": "Deduplicating rows",
    "section": "",
    "text": "Under the Name column, select Edit column -&gt; Add column based on this column…. Name this column temp.\nUsing GREL, we can access the value of other cells in the current row by using the cells variable and indicating which column, i.e., cells['Residence']. This object has multiple attributes, so we’ll want to specify that we want the string value, as in cells['Residence'].value.\nTo smoosh together several values, we can literally just “add” them together: cells['Name'].value + cells['Residence'].value + cells['Number in family'].value + cells['Remarks'].value.\nSort on this new column. In order to reorder the underlying data, select Sort -&gt; Reorder rows permanently from the top bar menu.\nNow we blank cells with the same values in our new temp column by selecting Edit cells -&gt; Blank down.\nNow facet on blank values by selecting Facet -&gt; Customized facets -&gt; Facet by blank. Then select the ‘true’ in the sidebar to limit to blank values.\nNow let’s mark all these rows temporarily. OpenRefine let’s us star any row or rows in order to mark them for further action. Select All -&gt; Edit rows -&gt; Star rows. Now go ahead and clear all facets in the left sidebar.\nFinally, to actually remove duplicate rows, select All -&gt; Facet -&gt;Facet by star. Now select All -&gt; Edit rows -&gt; Remove all matching rows.",
    "crumbs": [
      "Home",
      "Deduplicating rows"
    ]
  },
  {
    "objectID": "sections/deduplicating.html#creating-an-identifier",
    "href": "sections/deduplicating.html#creating-an-identifier",
    "title": "Deduplicating rows",
    "section": "",
    "text": "Once we’ve deduplicated, let’s not lose track of our unique rows or “observations.” As we learned in the last section, something fundamental to tidying data is being able to consistently identify observations (i.e., rows) across different tables. As we also learned in the last section, what we thought of as one table, if we are thinking to tidily, should be split into several according the question we’d like to answer.\nUnder the Name column, select Edit column -&gt; Add column based on this column…\nTo access the index of the current row, we can use the row variable and get the index value, i.e., row.index.\nOptionally, if you want to ensure each identifier is a four digit number, we can apply the fancier formula: \"0000\"[0,4-row.index.length()] + row.index\nNow select Edit column and move this new column to the beginning of the table.",
    "crumbs": [
      "Home",
      "Deduplicating rows"
    ]
  },
  {
    "objectID": "sections/exporting-data.html",
    "href": "sections/exporting-data.html",
    "title": "Exporting data",
    "section": "",
    "text": "As we mentioned earlier, rather than opening and saving individual files–an OpenRefine project (mostly) never changes the underlying data. Instead of saving over an existing version of a file, it “bundles” changes together, somewhat like git and GitHub. Rather than saving a file, we can export derivatives of the data according to whatever we need it to look like for a particular use.\n\n\nSelect the Undo / Redo tab in the left sidebar. Notice how every change we’ve made so far is recorded in a list. By selecting any one change in the list, we can go backwards or forwards in time.\n\n\n\nopenrefine undo/redo sidebar\n\n\nSelect the Extract option. We are presented with JSON of every change we made. We can add or delete steps by selecting/unselecting boxes. We can even copy-and-paste the JSON presented in the right column to another project or we can save it should we need to recover every step we took between versions of the data.\n\n\n\nopenrefine undo/redo extract menu\n\n\n\n\n\nTo create a derivative of the data select Export and notice the range of options. Select Custom tabular exporter and take a glance at the options available. The two we are going to focus on is the selection boxes with which we choose what columns. Note that we can drag the columns and change the order in which they appear. There is also a checkbox near the bottom right to Ignore facets and filters and export all rows. With this, we can choose to only export the data from the rows that appear with our currently active facets and filters.\n\n\n\nopenrefine export menu\n\n\nNow download a CSV with only the Id, Residence, Name, Street name, and Number in family columns, in that order. Note that you could also upload the spreadsheet straight to a Google Drive folder. This table – unexciting as it might seem – is the key to tidy data. With this key to unique residences that were sampled, we can create subsets of the data that answer specific questions like “How many households with children had females that were employed as washers?” or “How do men’s reported wages compare to women’s?” much more easily than before.",
    "crumbs": [
      "Home",
      "Exporting data"
    ]
  },
  {
    "objectID": "sections/exporting-data.html#undoredo",
    "href": "sections/exporting-data.html#undoredo",
    "title": "Exporting data",
    "section": "",
    "text": "Select the Undo / Redo tab in the left sidebar. Notice how every change we’ve made so far is recorded in a list. By selecting any one change in the list, we can go backwards or forwards in time.\n\n\n\nopenrefine undo/redo sidebar\n\n\nSelect the Extract option. We are presented with JSON of every change we made. We can add or delete steps by selecting/unselecting boxes. We can even copy-and-paste the JSON presented in the right column to another project or we can save it should we need to recover every step we took between versions of the data.\n\n\n\nopenrefine undo/redo extract menu",
    "crumbs": [
      "Home",
      "Exporting data"
    ]
  },
  {
    "objectID": "sections/exporting-data.html#export",
    "href": "sections/exporting-data.html#export",
    "title": "Exporting data",
    "section": "",
    "text": "To create a derivative of the data select Export and notice the range of options. Select Custom tabular exporter and take a glance at the options available. The two we are going to focus on is the selection boxes with which we choose what columns. Note that we can drag the columns and change the order in which they appear. There is also a checkbox near the bottom right to Ignore facets and filters and export all rows. With this, we can choose to only export the data from the rows that appear with our currently active facets and filters.\n\n\n\nopenrefine export menu\n\n\nNow download a CSV with only the Id, Residence, Name, Street name, and Number in family columns, in that order. Note that you could also upload the spreadsheet straight to a Google Drive folder. This table – unexciting as it might seem – is the key to tidy data. With this key to unique residences that were sampled, we can create subsets of the data that answer specific questions like “How many households with children had females that were employed as washers?” or “How do men’s reported wages compare to women’s?” much more easily than before.",
    "crumbs": [
      "Home",
      "Exporting data"
    ]
  },
  {
    "objectID": "sections/glossary.html",
    "href": "sections/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Tabular data - Anything that can be represented in cells, organized into columns and rows (like a spreadsheet).\nCSV and TSV - Common formats for tabular data. Cells are separated with commas (.csv or comma-separated values) or tabs (.tsv or tab-separated values).\nGraph data - Data that can be represented by nodes and edges or branching structures.\nJSON - (JavaScript Object Notation) A format for representing and storing graph data, designed for use with the C family of languages (including JavaScript).\nXML - (eXstensible Markup Language) A data format for representing graph data that uses syntax similar to HTML.\n\n\n\nTidy data - A specific system for cleaning and standardizing data for statistical analysis using the R programming language, developed by Hadley Wickham. See the original paper or an informal version.",
    "crumbs": [
      "Home",
      "Glossary"
    ]
  },
  {
    "objectID": "sections/glossary.html#data-types-and-structures",
    "href": "sections/glossary.html#data-types-and-structures",
    "title": "Glossary",
    "section": "",
    "text": "Tabular data - Anything that can be represented in cells, organized into columns and rows (like a spreadsheet).\nCSV and TSV - Common formats for tabular data. Cells are separated with commas (.csv or comma-separated values) or tabs (.tsv or tab-separated values).\nGraph data - Data that can be represented by nodes and edges or branching structures.\nJSON - (JavaScript Object Notation) A format for representing and storing graph data, designed for use with the C family of languages (including JavaScript).\nXML - (eXstensible Markup Language) A data format for representing graph data that uses syntax similar to HTML.",
    "crumbs": [
      "Home",
      "Glossary"
    ]
  },
  {
    "objectID": "sections/glossary.html#other-definitions",
    "href": "sections/glossary.html#other-definitions",
    "title": "Glossary",
    "section": "",
    "text": "Tidy data - A specific system for cleaning and standardizing data for statistical analysis using the R programming language, developed by Hadley Wickham. See the original paper or an informal version.",
    "crumbs": [
      "Home",
      "Glossary"
    ]
  },
  {
    "objectID": "sections/style-guide.html",
    "href": "sections/style-guide.html",
    "title": "Style Guide",
    "section": "",
    "text": "Three structural elements are important for an effective DRI session: scoping, scaffolding, and purpose. Scoping is the art of choosing what—and especially what not—to include in a session. Scaffolding is the process of building later skill development on prior skill development. Purpose provides an answer to the participant who asks, “Why should I care about this?”\n\n\nWhen leading a session, it’s tempting to want to comprehensively cover all important aspects of a tool, method, or skill. After all, this is an area you’re expert in, and every topic left out of the session is likely something you use frequently and find indispensable. However, in most cases, students will come away having learned more if a lesson is carefully scoped—that is, covering only carefully selected material. This allows the session to account for unexpected delays, which occur approximately 100% of the time. It also allows for review, question and answer, unstructured practice, and challenges. These approaches don’t need to be present in every lesson, but they can often improve outcomes for students.\nPeople tend to underestimate how long tasks will take in practice and to overestimate how much material it’s feasible to cover in a lesson. There’s a name for this phenomenon: the planning fallacy. For this reason, you should err on the side of planning to cover less material in a session. To make this easier, build in some challenges or unstructured activities to your lesson, ones that can be omitted or shortened if you’re running out of time. Often, you’ll find you don’t have time for these, but including them can increase your confidence in a well-scoped lesson.\n\n\n\nScaffolding is the art of structuring lessons so that students begin with more fundamental concepts or tasks and work towards increasingly difficult learning goals. With technical sessions, this often means finding ways to introduce concepts in isolation, so that students can come to terms with them without distraction from additional new or unfamiliar concepts. New methods and new vocabulary should be introduced explicitly rather than implicitly, and students should have time to integrate new knowledge before moving on to new information.\nWhen thinking about the structure of your lesson, consider what jargon or terms of art will be essential for students to know and plan where and how you’ll introduce those words or concepts. Don’t use jargon terms before introducing them.\nWhen planning a lesson, try to make your tooling and setup look as close to your students’ as possible. For example, if you have shell modifications, disable them before teaching. If that’s not possible, explain any differences students might be seeing. Remember that students are trying to match patterns in an unfamiliar and cognitively taxing environment and may not know what differences are significant and which are merely cosmetic.\nTry to incorporate review into your lesson organically. If a concept won’t be used repeatedly throughout your lesson, you might consider leaving it out altogether. Challenges or unstructured practice can also provide opportunities for review.\n\n\n\nThe most common question in DRI sessions is, “How does this relate to my research?” Researchers are more motivated to learn when they know that a tool or method can be used directly to advance their own goals. One way to help researchers to understand the purpose of a tool or method is to structure the lesson around a “deliverable”—a website, a piece of persuasive writing, a cleaned data set, an application, a visualization, or any other useful product. Even if the result does not relate directly to a participant’s research, it’s often easier to extrapolate usefulness from a concrete product than from a series of loosely coupled actions or skills.\nWorking toward creating a specific artifact in your lesson can also help with scaffolding and scoping. If you know exactly what students will need to learn to create, for example, a course website, that makes it easier to leave out concepts that don’t directly relate to that goal. It also suggests discrete steps and challenges that students must engage before completing the deliverable, often leading to a more intuitive lesson structure.\n\n\n\n\n\n\nMarkdown is a plain text markup format that is easy to read for both computers and humans. This is in contrast to, for example, HTML, which in its unrendered form is not enjoyable for humans to read. Compare the same document in HTML and markdown:\nHTML:\n&lt;h1&gt;My Book Report&lt;/h1&gt;\n  &lt;p&gt;&lt;i&gt;Persuasion&lt;/i&gt; is an &lt;strong&gt;excellent&lt;/strong&gt; book that I highly recommend for three reasons:&lt;/p&gt;\n  &lt;ul&gt;\n    &lt;li&gt;It's a comeback story.&lt;/li&gt;\n    &lt;li&gt;It has a catchy title.&lt;/li&gt;\n    &lt;li&gt;It's written by Jane Austen.&lt;/li&gt;\n  &lt;/ul&gt;\n  &lt;p&gt;You can buy the book &lt;a href=\"http://www.bookmonopoly.com/persuasion\"&gt;here.&lt;/a&gt;&lt;/p&gt;\nMarkdown:\n# My Book Report\n\n*Persuasion* is an **excellent** book that I highly recommend for three reasons:\n\n- It's a comeback story.\n- It has a catchy title.\n- It's written by Jane Austen.\n\nYou can buy the book [here](http://www.bookmonopoly.com/persuasion).\nWhen rendered by a browser, both of these documents look identical. However, the markdown document is considerably easier to read in its source form. While HTML is optimized to be readable by the computer, markdown is more balanced between the human and the machine.\n\n\n\nThe Fellows use markdown for tutorials for a number of reasons:\n\nIt’s exportable to other formats, such as HTML, PDF, and GitBook.\nIt can be kept under version control—with Git, for example.\nIt’s rendered automatically on GitHub.\nIt can be read locally with a text editor.\nAs plain text, it’s maintainable.\nIt’s easy to embed code and images.\nIt plays well with the tools we teach at the DRI.\n\nMany of markdown’s advantages come from the fact that it’s plain text. Tools that work well with code, such as version control, editors, and grep, also work well with markdown.\n\n\n\nMost formatting guidelines here are designed to make markdown source files more readable. Others are designed to preserve semantics, which allow markdown to be rendered the same in different contexts.\n\nSeperate paragraphs with a blank line.\nPlace blank lines between elements whenever possible. For example, put a blank line between a heading and a paragraph, and between a paragraph and an image link.\nUse headings consistently, and use them only to denote new sections. For example, do not use headings for emphasis.\nUse other markdown elements for their intended purpose. Use lists for lists and emphasis for emphasis. For example, do not use emphasis in place of a heading.\nUnless a raw URL is part of the tutorial, don’t leave raw URLs in your document. Incorporate the link into the flow of your prose.\nIf code segments are short, indicate them by indenting. If they’re longer, use the ````` syntax.\nWhen introducing new commands or code, put them on a new line so they stand out from the rest of the text.\nWhen including images, don’t leave the alt text segment [] blank. Fill it in with information that would be useful if the image could not be seen or rendered.\n\n\n\n\n\n\n\n\nIn general, it’s not necessary to provide in-line citations in the materials for a technical workshop unless you’re using someone else’s exact prose or you’re replicating a significant, non-trivial section of code. For example, code segments found on Stack Overflow do not need to be cited unless you think it’s pedagogically helpful. However, if you build your session around replicating a significant portion of a particular application, you must consider the license under which that software is released and consider providing attribution, even if attribution is not required under the terms of the license. In general, free and open source code licenses do not require attribution, but using a large portion of the code may require replicating the license. Check a summary of the terms of the license (or the license itself, if you’re feeling adventurous) if you decide to replicate a significant portion of an open-source project in your session."
  },
  {
    "objectID": "sections/style-guide.html#structure",
    "href": "sections/style-guide.html#structure",
    "title": "Style Guide",
    "section": "",
    "text": "Three structural elements are important for an effective DRI session: scoping, scaffolding, and purpose. Scoping is the art of choosing what—and especially what not—to include in a session. Scaffolding is the process of building later skill development on prior skill development. Purpose provides an answer to the participant who asks, “Why should I care about this?”\n\n\nWhen leading a session, it’s tempting to want to comprehensively cover all important aspects of a tool, method, or skill. After all, this is an area you’re expert in, and every topic left out of the session is likely something you use frequently and find indispensable. However, in most cases, students will come away having learned more if a lesson is carefully scoped—that is, covering only carefully selected material. This allows the session to account for unexpected delays, which occur approximately 100% of the time. It also allows for review, question and answer, unstructured practice, and challenges. These approaches don’t need to be present in every lesson, but they can often improve outcomes for students.\nPeople tend to underestimate how long tasks will take in practice and to overestimate how much material it’s feasible to cover in a lesson. There’s a name for this phenomenon: the planning fallacy. For this reason, you should err on the side of planning to cover less material in a session. To make this easier, build in some challenges or unstructured activities to your lesson, ones that can be omitted or shortened if you’re running out of time. Often, you’ll find you don’t have time for these, but including them can increase your confidence in a well-scoped lesson.\n\n\n\nScaffolding is the art of structuring lessons so that students begin with more fundamental concepts or tasks and work towards increasingly difficult learning goals. With technical sessions, this often means finding ways to introduce concepts in isolation, so that students can come to terms with them without distraction from additional new or unfamiliar concepts. New methods and new vocabulary should be introduced explicitly rather than implicitly, and students should have time to integrate new knowledge before moving on to new information.\nWhen thinking about the structure of your lesson, consider what jargon or terms of art will be essential for students to know and plan where and how you’ll introduce those words or concepts. Don’t use jargon terms before introducing them.\nWhen planning a lesson, try to make your tooling and setup look as close to your students’ as possible. For example, if you have shell modifications, disable them before teaching. If that’s not possible, explain any differences students might be seeing. Remember that students are trying to match patterns in an unfamiliar and cognitively taxing environment and may not know what differences are significant and which are merely cosmetic.\nTry to incorporate review into your lesson organically. If a concept won’t be used repeatedly throughout your lesson, you might consider leaving it out altogether. Challenges or unstructured practice can also provide opportunities for review.\n\n\n\nThe most common question in DRI sessions is, “How does this relate to my research?” Researchers are more motivated to learn when they know that a tool or method can be used directly to advance their own goals. One way to help researchers to understand the purpose of a tool or method is to structure the lesson around a “deliverable”—a website, a piece of persuasive writing, a cleaned data set, an application, a visualization, or any other useful product. Even if the result does not relate directly to a participant’s research, it’s often easier to extrapolate usefulness from a concrete product than from a series of loosely coupled actions or skills.\nWorking toward creating a specific artifact in your lesson can also help with scaffolding and scoping. If you know exactly what students will need to learn to create, for example, a course website, that makes it easier to leave out concepts that don’t directly relate to that goal. It also suggests discrete steps and challenges that students must engage before completing the deliverable, often leading to a more intuitive lesson structure."
  },
  {
    "objectID": "sections/style-guide.html#markdown-guidelines",
    "href": "sections/style-guide.html#markdown-guidelines",
    "title": "Style Guide",
    "section": "",
    "text": "Markdown is a plain text markup format that is easy to read for both computers and humans. This is in contrast to, for example, HTML, which in its unrendered form is not enjoyable for humans to read. Compare the same document in HTML and markdown:\nHTML:\n&lt;h1&gt;My Book Report&lt;/h1&gt;\n  &lt;p&gt;&lt;i&gt;Persuasion&lt;/i&gt; is an &lt;strong&gt;excellent&lt;/strong&gt; book that I highly recommend for three reasons:&lt;/p&gt;\n  &lt;ul&gt;\n    &lt;li&gt;It's a comeback story.&lt;/li&gt;\n    &lt;li&gt;It has a catchy title.&lt;/li&gt;\n    &lt;li&gt;It's written by Jane Austen.&lt;/li&gt;\n  &lt;/ul&gt;\n  &lt;p&gt;You can buy the book &lt;a href=\"http://www.bookmonopoly.com/persuasion\"&gt;here.&lt;/a&gt;&lt;/p&gt;\nMarkdown:\n# My Book Report\n\n*Persuasion* is an **excellent** book that I highly recommend for three reasons:\n\n- It's a comeback story.\n- It has a catchy title.\n- It's written by Jane Austen.\n\nYou can buy the book [here](http://www.bookmonopoly.com/persuasion).\nWhen rendered by a browser, both of these documents look identical. However, the markdown document is considerably easier to read in its source form. While HTML is optimized to be readable by the computer, markdown is more balanced between the human and the machine.\n\n\n\nThe Fellows use markdown for tutorials for a number of reasons:\n\nIt’s exportable to other formats, such as HTML, PDF, and GitBook.\nIt can be kept under version control—with Git, for example.\nIt’s rendered automatically on GitHub.\nIt can be read locally with a text editor.\nAs plain text, it’s maintainable.\nIt’s easy to embed code and images.\nIt plays well with the tools we teach at the DRI.\n\nMany of markdown’s advantages come from the fact that it’s plain text. Tools that work well with code, such as version control, editors, and grep, also work well with markdown.\n\n\n\nMost formatting guidelines here are designed to make markdown source files more readable. Others are designed to preserve semantics, which allow markdown to be rendered the same in different contexts.\n\nSeperate paragraphs with a blank line.\nPlace blank lines between elements whenever possible. For example, put a blank line between a heading and a paragraph, and between a paragraph and an image link.\nUse headings consistently, and use them only to denote new sections. For example, do not use headings for emphasis.\nUse other markdown elements for their intended purpose. Use lists for lists and emphasis for emphasis. For example, do not use emphasis in place of a heading.\nUnless a raw URL is part of the tutorial, don’t leave raw URLs in your document. Incorporate the link into the flow of your prose.\nIf code segments are short, indicate them by indenting. If they’re longer, use the ````` syntax.\nWhen introducing new commands or code, put them on a new line so they stand out from the rest of the text.\nWhen including images, don’t leave the alt text segment [] blank. Fill it in with information that would be useful if the image could not be seen or rendered."
  },
  {
    "objectID": "sections/style-guide.html#free-and-open-source-code",
    "href": "sections/style-guide.html#free-and-open-source-code",
    "title": "Style Guide",
    "section": "",
    "text": "In general, it’s not necessary to provide in-line citations in the materials for a technical workshop unless you’re using someone else’s exact prose or you’re replicating a significant, non-trivial section of code. For example, code segments found on Stack Overflow do not need to be cited unless you think it’s pedagogically helpful. However, if you build your session around replicating a significant portion of a particular application, you must consider the license under which that software is released and consider providing attribution, even if attribution is not required under the terms of the license. In general, free and open source code licenses do not require attribution, but using a large portion of the code may require replicating the license. Check a summary of the terms of the license (or the license itself, if you’re feeling adventurous) if you decide to replicate a significant portion of an open-source project in your session."
  },
  {
    "objectID": "sections/tidy-vs-messy-ii.html",
    "href": "sections/tidy-vs-messy-ii.html",
    "title": "Tidy vs. Messy Part II",
    "section": "",
    "text": "So what, again, is tidy data? More than taking a stand against error-riddled, inconsistent data, tidy refers to an attempt to standardize how we might organize data. Now that we’ve had a chance to explore a single research dataset in more detail, we are better equipped to explore how to apply tidy data concepts. According to Wickham:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nMessy data, on the other hand, is everything else.\n\n\nModeling real world phenomena in data is hard, incomplete, and often problematic. That does not preclude being able to identify large classes of issues that makes working with data significantly easier. Extrapolating from the above three concepts, five common issues are:\n\nColumn headers should be variable names, not values.\nOne column should store values for one variable.\nVariables should be stored in columns, not rows.\nDifferent types of observational units should appear in different tables.\nA single observational unit should appear in a single table.\n\nGuidelines like these can be incredibly helpful for our sanity as we try to decode historical data or data created by hand. They can also be useful in guiding those of us who are not data scientists but want to create datasets from our research.\nThe following examples derive from widely shared examples initially used by Hadley Wickham for teaching purposes. For a fuller treatment, see Hadley Wiksham’s teaching slide deck as well as the video version of these concepts.\n\n\n\n\n\n\nreligion\n&lt;$10k\n$10-20k\n$20-30k\n$30-40k\n\n\n\n\n1\nAgnostic\n27\n34\n60\n81\n\n\n2\nAtheist\n12\n27\n37\n52\n\n\n3\nBuddhist\n27\n21\n30\n34\n\n\n4\nCatholic\n418\n617\n732\n670\n\n\n5\nDon’t know/refused\n15\n14\n15\n11\n\n\n\nIn this excerpted table of income distribution between religious denominations from the Pew Foundation, where might headers be actually harboring values instead of variable names?\n\n\n\n\nreligion\nincome\ncount\n\n\n\n\n1\nagnostic\n&lt;$10k\n27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\nyear\nm014\nm1524\nm2534\nm3544\nm4554\nm5564\nm65\nmu\nf014\nf1524\nf2534\nf3544\nf4554\nf5564\nf65\nfu\n\n\n\n\nUS\n1995\n19\n355\n876\n1417\n1121\n742\n1099\nNA\n26\n280\n579\n499\n285\n202\n591\nNA\n\n\n\nVariables include sex (m, f) and age (0–14, 15–25, 25–34, 35–44, 45–54, 55–64, over 65, unknown).\nIn this excerpt of tuberculosis dataset from the World Health Organization, where can you see multiple variable crammed into one column?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nyear\nmonth\nelement\nd1\nd2\nd3\nd4\nd5\nd6\nd7\nd8\nd9\nd10\nd11\nd12\nd13\nd14\nd15\nd16\nd17\nd18\nd19\nd20\nd21\nd22\nd23\nd24\nd25\nd26\nd27\nd28\nd29\nd30\nd31\n\n\n\n\nMX000017004\n2010\n1\nTMAX\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n278\nNA\n\n\nMX000017004\n2010\n1\nTMIN\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n145\nNA\n\n\nMX000017004\n2010\n2\nTMAX\nNA\n273\n241\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n297\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n299\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nMX000017004\n2010\n2\nTMIN\nNA\n144\n144\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n134\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n107\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nIn this excerpt of weather data from Cuernavaca, Mexico, what variable seems to appear in both a column and a row?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nartist\ntrack\ntime\ndate.entered\nwk1\nwk2\nwk3\n\n\n\n\n2000\n2Ge+her\nThe Hardest Part Of …\n3:15\n2000-09-02\n91\n87\n92\n\n\n2000\n3 Doors\nDown Kryptonite\n3:53\n2000-04-08\n81\n70\n68\n\n\n2000\n98^0 Give\nMe Just One Nig…\n3:24\n2000-08-19\n51\n39\n34\n\n\n2000\nA*Teens\nDancing Queen\n3:44\n2000-07-08\n97\n97\n96\n\n\n2000\nAaliyah\nI Don’t Wanna\n4:15\n2000-01-29\n84\n62\n51\n\n\n2000\nAaliyah\nTry Again\n4:03\n2000-03-18\n59\n53\n38\n\n\n2000\nAdams, Yolanda\nOpen My Heart\n5:30\n2000-08-26\n76\n76\n74\n\n\n\nBillboard Chart data from 2000 – notice how the date is spread across different columns; however, if each row is a song is there any better way to do this?\nAn intermediate step is to make a new row for each song each week, like so.\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nartist\ntrack\ntime\ndate\nweek\nrank\n\n\n\n\n2000\n2Ge+her\nThe Hardest Part Of…\n3:15\n2000-09-02\n1\n91\n\n\n2000\n2Ge+her\nThe Hardest Part Of…\n3:15\n2000-09-09\n2\n87\n\n\n2000\n2Ge+her\nThe Hardest Part Of…\n3:15\n2000-09-16\n3\n92\n\n\n2000\n2 Pac\nBaby Don’t Cry\n4:22\n2000-02-26\n1\n87\n\n\n2000\n2 Pac\nBaby Don’t Cry\n4:22\n2000-03-04\n2\n82\n\n\n2000\n2 Pac\nBaby Don’t Cry\n4:22\n2000-03-11\n3\n72\n\n\n2000\n2 Pac\nBaby Don’t Cry\n4:22\n2000-03-18\n4\n77\n\n\n2000\n2 Pac\nBaby Don’t Cry\n4:22\n2000-03-25\n5\n87\n\n\n2000\n2 Pac\nBaby Don’t Cry\n4:22\n2000-04-01\n6\n94\n\n\n2000\n2 Pac\nBaby Don’t Cry\n4:22\n2000-04-08\n7\n99\n\n\n2000\n3 Doors Down\nKryptonite\n3:53\n2000-04-08\n1\n81\n\n\n2000\n3 Doors Down\nKryptonite\n3:53\n2000-04-15\n2\n70\n\n\n2000\n3 Doors Down\nKryptonite\n3:53\n2000-04-22\n3\n68\n\n\n2000\n3 Doors Down\nKryptonite\n3:53\n2000-04-29\n4\n67\n\n\n2000\n3 Doors Down\nKryptonite\n3:53\n2000-05-06\n5\n66\n\n\n\nThere are still issues, though. For example, notice how many times we repeat the basic metadata of each song (year, artist, time).\nThe “tidy” way to address this is by considering this in terms of “observations” and differing observational units. In this case, “observational unit” mostly means the row. If we split this into two tables, one whose unit is the song and one whose unit is a song’s rank from week to week, here’s what it might look like.\nBillboard Table 1\n\n\n\nid\nartist\ntrack\ntime\n\n\n\n\n1\n2 Pac Baby\nDon’t Cry\n4:22\n\n\n2\n2Ge+her\nThe Hardest Part Of …\n3:15\n\n\n3\n3 Doors Down\nKryptonite\n3:53\n\n\n4\n3 Doors Down\nLoser\n4:24\n\n\n5\n504 Boyz\nWobble Wobble\n3:35\n\n\n6\n98^0\nGive Me Just One Nig…\n3:24\n\n\n7\nA*Teens\nDancing Queen\n3:44\n\n\n8\nAaliyah\nI Don’t Wanna\n4:15\n\n\n9\nAaliyah\nTry Again\n4:03\n\n\n10\nAdams, Yolanda\nOpen My Heart\n5:30\n\n\n\nBillboard Table 2\n\n\n\nid\ndate\nrank\n\n\n\n\n1\n2000-02-26\n87\n\n\n1\n2000-03-04\n82\n\n\n1\n2000-03-11\n72\n\n\n1\n2000-03-18\n77\n\n\n1\n2000-03-25\n87\n\n\n1\n2000-04-01\n94\n\n\n1\n2000-04-08\n99\n\n\n2\n2000-09-02\n91\n\n\n2\n2000-09-09\n87\n\n\n2\n2000-09-16\n92\n\n\n\n\n\n\nThough we won’t delve into this, for completeness, we’ll mention that another common issue is observational units across multiple tables. Simply put, it’s often the case that all the “observations” or rows may not all be in the same table. Imagine census data in which different counties are stored in separate files or the details of a historical trading voyage stored in different ledgers, possibly in different archives. The tidy way to deal with this before analysis, would be to combine them into a single table or file.\n\n\n\n\nLooking at our 1847 census dataset, can you find an example of the first four issues? What would it take to tidy them up?\n\nColumn headers should be variables not values.\nOne column should describe one variable.\nVariables should be in columns not rows.\nDifferent observational units should appear in different tables.",
    "crumbs": [
      "Home",
      "Tidy vs. Messy part II"
    ]
  },
  {
    "objectID": "sections/tidy-vs-messy-ii.html#common-issues",
    "href": "sections/tidy-vs-messy-ii.html#common-issues",
    "title": "Tidy vs. Messy Part II",
    "section": "",
    "text": "Modeling real world phenomena in data is hard, incomplete, and often problematic. That does not preclude being able to identify large classes of issues that makes working with data significantly easier. Extrapolating from the above three concepts, five common issues are:\n\nColumn headers should be variable names, not values.\nOne column should store values for one variable.\nVariables should be stored in columns, not rows.\nDifferent types of observational units should appear in different tables.\nA single observational unit should appear in a single table.\n\nGuidelines like these can be incredibly helpful for our sanity as we try to decode historical data or data created by hand. They can also be useful in guiding those of us who are not data scientists but want to create datasets from our research.\nThe following examples derive from widely shared examples initially used by Hadley Wickham for teaching purposes. For a fuller treatment, see Hadley Wiksham’s teaching slide deck as well as the video version of these concepts.\n\n\n\n\n\n\nreligion\n&lt;$10k\n$10-20k\n$20-30k\n$30-40k\n\n\n\n\n1\nAgnostic\n27\n34\n60\n81\n\n\n2\nAtheist\n12\n27\n37\n52\n\n\n3\nBuddhist\n27\n21\n30\n34\n\n\n4\nCatholic\n418\n617\n732\n670\n\n\n5\nDon’t know/refused\n15\n14\n15\n11\n\n\n\nIn this excerpted table of income distribution between religious denominations from the Pew Foundation, where might headers be actually harboring values instead of variable names?\n\n\n\n\nreligion\nincome\ncount\n\n\n\n\n1\nagnostic\n&lt;$10k\n27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\nyear\nm014\nm1524\nm2534\nm3544\nm4554\nm5564\nm65\nmu\nf014\nf1524\nf2534\nf3544\nf4554\nf5564\nf65\nfu\n\n\n\n\nUS\n1995\n19\n355\n876\n1417\n1121\n742\n1099\nNA\n26\n280\n579\n499\n285\n202\n591\nNA\n\n\n\nVariables include sex (m, f) and age (0–14, 15–25, 25–34, 35–44, 45–54, 55–64, over 65, unknown).\nIn this excerpt of tuberculosis dataset from the World Health Organization, where can you see multiple variable crammed into one column?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nyear\nmonth\nelement\nd1\nd2\nd3\nd4\nd5\nd6\nd7\nd8\nd9\nd10\nd11\nd12\nd13\nd14\nd15\nd16\nd17\nd18\nd19\nd20\nd21\nd22\nd23\nd24\nd25\nd26\nd27\nd28\nd29\nd30\nd31\n\n\n\n\nMX000017004\n2010\n1\nTMAX\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n278\nNA\n\n\nMX000017004\n2010\n1\nTMIN\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n145\nNA\n\n\nMX000017004\n2010\n2\nTMAX\nNA\n273\n241\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n297\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n299\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nMX000017004\n2010\n2\nTMIN\nNA\n144\n144\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n134\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n107\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nIn this excerpt of weather data from Cuernavaca, Mexico, what variable seems to appear in both a column and a row?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nartist\ntrack\ntime\ndate.entered\nwk1\nwk2\nwk3\n\n\n\n\n2000\n2Ge+her\nThe Hardest Part Of …\n3:15\n2000-09-02\n91\n87\n92\n\n\n2000\n3 Doors\nDown Kryptonite\n3:53\n2000-04-08\n81\n70\n68\n\n\n2000\n98^0 Give\nMe Just One Nig…\n3:24\n2000-08-19\n51\n39\n34\n\n\n2000\nA*Teens\nDancing Queen\n3:44\n2000-07-08\n97\n97\n96\n\n\n2000\nAaliyah\nI Don’t Wanna\n4:15\n2000-01-29\n84\n62\n51\n\n\n2000\nAaliyah\nTry Again\n4:03\n2000-03-18\n59\n53\n38\n\n\n2000\nAdams, Yolanda\nOpen My Heart\n5:30\n2000-08-26\n76\n76\n74\n\n\n\nBillboard Chart data from 2000 – notice how the date is spread across different columns; however, if each row is a song is there any better way to do this?\nAn intermediate step is to make a new row for each song each week, like so.\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nartist\ntrack\ntime\ndate\nweek\nrank\n\n\n\n\n2000\n2Ge+her\nThe Hardest Part Of…\n3:15\n2000-09-02\n1\n91\n\n\n2000\n2Ge+her\nThe Hardest Part Of…\n3:15\n2000-09-09\n2\n87\n\n\n2000\n2Ge+her\nThe Hardest Part Of…\n3:15\n2000-09-16\n3\n92\n\n\n2000\n2 Pac\nBaby Don’t Cry\n4:22\n2000-02-26\n1\n87\n\n\n2000\n2 Pac\nBaby Don’t Cry\n4:22\n2000-03-04\n2\n82\n\n\n2000\n2 Pac\nBaby Don’t Cry\n4:22\n2000-03-11\n3\n72\n\n\n2000\n2 Pac\nBaby Don’t Cry\n4:22\n2000-03-18\n4\n77\n\n\n2000\n2 Pac\nBaby Don’t Cry\n4:22\n2000-03-25\n5\n87\n\n\n2000\n2 Pac\nBaby Don’t Cry\n4:22\n2000-04-01\n6\n94\n\n\n2000\n2 Pac\nBaby Don’t Cry\n4:22\n2000-04-08\n7\n99\n\n\n2000\n3 Doors Down\nKryptonite\n3:53\n2000-04-08\n1\n81\n\n\n2000\n3 Doors Down\nKryptonite\n3:53\n2000-04-15\n2\n70\n\n\n2000\n3 Doors Down\nKryptonite\n3:53\n2000-04-22\n3\n68\n\n\n2000\n3 Doors Down\nKryptonite\n3:53\n2000-04-29\n4\n67\n\n\n2000\n3 Doors Down\nKryptonite\n3:53\n2000-05-06\n5\n66\n\n\n\nThere are still issues, though. For example, notice how many times we repeat the basic metadata of each song (year, artist, time).\nThe “tidy” way to address this is by considering this in terms of “observations” and differing observational units. In this case, “observational unit” mostly means the row. If we split this into two tables, one whose unit is the song and one whose unit is a song’s rank from week to week, here’s what it might look like.\nBillboard Table 1\n\n\n\nid\nartist\ntrack\ntime\n\n\n\n\n1\n2 Pac Baby\nDon’t Cry\n4:22\n\n\n2\n2Ge+her\nThe Hardest Part Of …\n3:15\n\n\n3\n3 Doors Down\nKryptonite\n3:53\n\n\n4\n3 Doors Down\nLoser\n4:24\n\n\n5\n504 Boyz\nWobble Wobble\n3:35\n\n\n6\n98^0\nGive Me Just One Nig…\n3:24\n\n\n7\nA*Teens\nDancing Queen\n3:44\n\n\n8\nAaliyah\nI Don’t Wanna\n4:15\n\n\n9\nAaliyah\nTry Again\n4:03\n\n\n10\nAdams, Yolanda\nOpen My Heart\n5:30\n\n\n\nBillboard Table 2\n\n\n\nid\ndate\nrank\n\n\n\n\n1\n2000-02-26\n87\n\n\n1\n2000-03-04\n82\n\n\n1\n2000-03-11\n72\n\n\n1\n2000-03-18\n77\n\n\n1\n2000-03-25\n87\n\n\n1\n2000-04-01\n94\n\n\n1\n2000-04-08\n99\n\n\n2\n2000-09-02\n91\n\n\n2\n2000-09-09\n87\n\n\n2\n2000-09-16\n92\n\n\n\n\n\n\nThough we won’t delve into this, for completeness, we’ll mention that another common issue is observational units across multiple tables. Simply put, it’s often the case that all the “observations” or rows may not all be in the same table. Imagine census data in which different counties are stored in separate files or the details of a historical trading voyage stored in different ledgers, possibly in different archives. The tidy way to deal with this before analysis, would be to combine them into a single table or file.",
    "crumbs": [
      "Home",
      "Tidy vs. Messy part II"
    ]
  },
  {
    "objectID": "sections/tidy-vs-messy-ii.html#challenge",
    "href": "sections/tidy-vs-messy-ii.html#challenge",
    "title": "Tidy vs. Messy Part II",
    "section": "",
    "text": "Looking at our 1847 census dataset, can you find an example of the first four issues? What would it take to tidy them up?\n\nColumn headers should be variables not values.\nOne column should describe one variable.\nVariables should be in columns not rows.\nDifferent observational units should appear in different tables.",
    "crumbs": [
      "Home",
      "Tidy vs. Messy part II"
    ]
  },
  {
    "objectID": "sections/transforming-columns.html",
    "href": "sections/transforming-columns.html",
    "title": "Transforming columns",
    "section": "",
    "text": "Transforming columns\n\nSplitting columns\nIf you apply a text facet to Occupation of females compensation, notice how many rows include a wage. This is classic messy data, including multiple data types (categorical and discontinuous) under the same variable. One way to begin examining and comparing wages would be to separate those wages into a separate column.\nUnder Occupation of females compensation, select Edit column -&gt; Split into several columns.\nMost commonly, multiple values are separated by a character like a comma. In this case, we see the $ character.\nReplace the comma with a $ in the Separator field. Note we can limit the maximum number of columns – this could be important in case very very messy data!\nAdditionally, we’ll want to preserve the current column – so uncheck the Remove this column option. Select OK.\nNow we should have a few new columns. We expect the first new column to be unhelpful – this column will include text in each cell before the $ dollar sign. The next column will be our first column of wages. What are the other columns that were created?\nLet’s delete the first new column by selecting Edit column -&gt; Remove this column.\nNow let’s rename the rest of the new columns by selecting Edit column -&gt; Rename this column to ‘Female wages’. Apply a Text facet to our first Female wages column, what do you notice?\nThis is a beginning, but you might have noticed that not all the cells with wages under Occupation of females compensation included $ dollar signs. What if we could make sure to select all Arabic numerals, not just for cells that happened to include $ dollar signs?\n\n\nChallenge\nFrom how many residences on Lombard St. was data collected? Or from any other street? Let’s explore one path to answering such a question that makes use of OpenRefine’s more advanced features.\nUnder the Residence column, selecting Text facet isn’t super helpful because it doesn’t aggregate different addresses, partly because of address numbers. What if we could strip away address numbers?\n\n\n\nadd column based on this column openrefine menu\n\n\nUnder the Residence column, select Edit column -&gt; Add column based on this column….\nAdd a new column name, in this case ‘Street name’. For our purposes, we will be using General Refine Expression Language (note the options for Python or Clojure in the dropdown menu). As we enter text in the resulting prompt, we can see results update live.\nStart by entering value. Next, let’s try to find a particular value – say the number ‘144’. We would do this by using the method find, as in value.find('144').\nHow about any number? We can use a commonly used shorthand code called regular expressions to match whole classes of characters, in this case Arabic numerals.\nFirst we need to indicate we are going to use a regular expression instead of normal text. We do this with / slashes instead of quotation marks.\nThe shorthand for one Arabic numeral is \\d. The shorthand for any number of Arabic numerals together is \\d+.\nWhat about ‘8th’ as in 8th Street? In order to save those numerals, we’ll want to include a space after.\nThis way we will only match on a numeral or a string of numerals followed by a space. So now we have value.find(/\\d+ /).\nFinally, we wanted to get rid of the numbers, remember? So we’ll swap the .find method for .replace. What should we replace the digits with? Nothing, a blank, which we can indicate with empty quotation marks '' – as in value.replace(/\\d+ /, '').\nSelect OK and we should have a new column with numbers stripped.\nIn our new column Street name, select Facet -&gt; Text facet. Next select count besides Sort by.\nSo how many residences surveyed were on Lombard Street? Clearly, this is not perfect. What step might we take to better cluster addresses based on the street?",
    "crumbs": [
      "Home",
      "Transforming columns"
    ]
  }
]